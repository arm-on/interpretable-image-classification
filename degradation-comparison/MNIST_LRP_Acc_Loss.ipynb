{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P5 - MNIST - LRP - Acc Loss.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIFO7VVPsarJ"
      },
      "source": [
        "Google Drive Authentication\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd9Bga-O7rxK",
        "outputId": "41bf4cf5-d832-44c4-e6f6-1d18e19ad7a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "%cd /gdrive/My Drive/IIC/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive/DrRabiee-Project/Refactor/P3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_Wnv6dofH5Z"
      },
      "source": [
        "Importing/Installing the Necessary Libraries\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Pk39vYTniR",
        "outputId": "90bd0eb1-7f7f-4ae9-cca1-211eafc4005a"
      },
      "source": [
        "!pip uninstall h5py\n",
        "!pip install h5py==2.10.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling h5py-3.1.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py-3.1.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py.libs/libaec-9c9e97eb.so.0.0.10\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py.libs/libhdf5-00e8fae8.so.200.0.0\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py.libs/libhdf5_hl-383c339f.so.200.0.0\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py.libs/libsz-e7aa62f5.so.2.0.1\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py.libs/libz-eb09ad1d.so.1.2.3\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled h5py-3.1.0\n",
            "Collecting h5py==2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: h5py\n",
            "Successfully installed h5py-2.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud5uXXCYmmaz"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOgIWpcamoP3",
        "outputId": "c088a55e-dbd8-4b2e-8a9e-fde584a9ef1e"
      },
      "source": [
        "pip install innvestigate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting innvestigate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/5f/d8c1c0136ddaf0720cb663edf6c2361f54d499283b813c1823e5343aff81/innvestigate-1.0.9-py3-none-any.whl (100kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from innvestigate) (2.10.0)\n",
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from innvestigate) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from innvestigate) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from innvestigate) (0.16.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from innvestigate) (3.6.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from innvestigate) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->innvestigate) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4->innvestigate) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4->innvestigate) (3.13)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->innvestigate) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->innvestigate) (8.8.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->innvestigate) (21.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->innvestigate) (57.0.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->innvestigate) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->innvestigate) (0.7.1)\n",
            "Installing collected packages: keras-applications, keras, innvestigate\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed innvestigate-1.0.9 keras-2.2.4 keras-applications-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6axYEo-tmp3F",
        "outputId": "5e2fa5f6-0850-4e78-cb9f-b87801de35cd"
      },
      "source": [
        "%matplotlib inline  \n",
        "%tensorflow_version 1.x\n",
        "import imp\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import keras\n",
        "import keras.backend\n",
        "import keras.models\n",
        "\n",
        "import innvestigate\n",
        "import innvestigate.utils as iutils\n",
        "\n",
        "# Use utility libraries to focus on relevant iNNvestigate routines.\n",
        "eutils = imp.load_source(\"utils\", \"utils.py\")\n",
        "mnistutils = imp.load_source(\"utils_mnist\", \"utils_mnist.py\")\n",
        "from keras.models import model_from_json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0uADhY875vx"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import bottleneck as bn\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yBkN5ai8FSy"
      },
      "source": [
        "from skimage.color import rgb2gray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL1ZovSFAxET"
      },
      "source": [
        "Data Preparation\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev0ePCu3As46",
        "outputId": "28c2ed87-b7de-4ba0-9e63-739a7475657c"
      },
      "source": [
        "from skimage.color import gray2rgb\n",
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 3)\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 3)\n",
        "x_train = gray2rgb(x_train.reshape(x_train.shape[0],28,28))\n",
        "x_test = gray2rgb(x_test.reshape(x_test.shape[0],28,28))\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n",
            "x_train shape: (60000, 28, 28, 3)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9KE7MyWyfxi"
      },
      "source": [
        "data = (x_train, y_train, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixKAo8A4Vb6r"
      },
      "source": [
        "num_classes = len(np.unique(data[1]))\n",
        "label_to_class_name = [str(i) for i in range(num_classes)]\n",
        "input_range = [-1, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTovjqJyV7Gd"
      },
      "source": [
        "Loading the Model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roDkYf9NS3TX",
        "outputId": "19617d12-4565-4b96-bf35-c7f44e1bcb59"
      },
      "source": [
        "json_file = open('models/mnist_model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "model.load_weights(\"models/mnist_model.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmGdZWqNe0mS"
      },
      "source": [
        "Layer-wise Relevance Propagation\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3svo1dSmvzB"
      },
      "source": [
        "def preprocessing(X):\n",
        "    return X\n",
        "def revert_preprocessing(X):\n",
        "    return X\n",
        "def input_preprocessing(X):\n",
        "    return X\n",
        "def input_postprocessing(X):\n",
        "    return X\n",
        "\n",
        "noise_scale = (input_range[1]-input_range[0]) * 0.1\n",
        "ri = input_range[0]  # reference input\n",
        "\n",
        "\n",
        "# Configure analysis methods and properties\n",
        "methods = [\n",
        "    # NAME                    OPT.PARAMS                POSTPROC FXN               TITLE\n",
        "\n",
        "    # Show input\n",
        "    (\"input\",                 {},                       input_postprocessing,      \"Input\"),\n",
        "\n",
        "    (\"lrp.z\",                 {},                       mnistutils.heatmap,        \"LRP-Z\"),\n",
        "    (\"lrp.epsilon\",           {\"epsilon\": 1},           mnistutils.heatmap,        \"LRP-Epsilon\"),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7QWDSePmzTU"
      },
      "source": [
        "# Create model without trailing softmax\n",
        "model_wo_softmax = iutils.keras.graph.model_wo_softmax(model)\n",
        "\n",
        "# Create analyzers.\n",
        "analyzers = []\n",
        "for method in methods:\n",
        "    analyzer = innvestigate.create_analyzer(method[0],        # analysis method identifier\n",
        "                                            model_wo_softmax, # model without softmax output\n",
        "                                            **method[1])      # optional analysis parameters\n",
        "\n",
        "    # Some analyzers require training.\n",
        "    analyzer.fit(x_train, batch_size=256, verbose=1)\n",
        "    # analyzer.fit(data[0], batch_size=256, verbose=1)\n",
        "    analyzers.append(analyzer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHVyPiAQ7bmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d706ed12-e4db-4bea-a5ae-598c1bb177b9"
      },
      "source": [
        "%%capture\n",
        "\n",
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "\n",
        "num_samples = 1000\n",
        "indices = [i for i in range(num_samples)]\n",
        "test_images = list(zip(data[2][indices], data[3][indices]))\n",
        "\n",
        "analysis = np.zeros([len(test_images), len(analyzers), 28, 28, 3])\n",
        "text = []\n",
        "\n",
        "\n",
        "for i, (x, y) in enumerate(test_images):\n",
        "    # Add batch axis.\n",
        "    x = x[None, :, :, :]\n",
        "    \n",
        "    # Predict final activations, probabilites, and label.\n",
        "    presm = model_wo_softmax.predict_on_batch(x)[0]\n",
        "    prob = model.predict_on_batch(x)[0]\n",
        "    y_hat = prob.argmax()\n",
        "    \n",
        "    # Save prediction info:\n",
        "    text.append((\"%s\" % label_to_class_name[y],    # ground truth label\n",
        "                 \"%.2f\" % presm.max(),             # pre-softmax logits\n",
        "                 \"%.2f\" % prob.max(),              # probabilistic softmax output  \n",
        "                 \"%s\" % label_to_class_name[y_hat] # predicted label\n",
        "                ))\n",
        "\n",
        "    \n",
        "    for aidx, analyzer in enumerate(analyzers):\n",
        "        # Analyze.\n",
        "        a = analyzer.analyze(x)\n",
        "        \n",
        "        # Apply common postprocessing, e.g., re-ordering the channels for plotting.\n",
        "        a = mnistutils.postprocess(a)\n",
        "        # Apply analysis postprocessing, e.g., creating a heatmap.\n",
        "        a = methods[aidx][2](a)\n",
        "        # Store the analysis.\n",
        "        analysis[i, aidx] = a[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnb7behy_qhA"
      },
      "source": [
        "Image Desctruction: Adding Pepper Noise to the Images such that only 20 percent of the most important pixels of each one remains.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynqO2LIG8WPy"
      },
      "source": [
        "# the following function gets the top n elements (in terms of magnitude) from a numpy array\n",
        "def top_n_indexes(arr, n):\n",
        "    idx = bn.argpartition(arr, arr.size-n, axis=None)[-n:]\n",
        "    width = arr.shape[1]\n",
        "    return [divmod(i, width) for i in idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X0-uEEQ8Wiv"
      },
      "source": [
        "# given a list of tuples (pixel indices), this function generates a list of other pixels\n",
        "def get_other_pixels(idx_list):\n",
        "    idx_list = set(idx_list)\n",
        "    other_pixels = []\n",
        "    for i in range(28):\n",
        "        for j in range(28):\n",
        "            if (i,j) not in idx_list:\n",
        "                other_pixels.append((i,j))\n",
        "    return other_pixels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW6hYo198YCG"
      },
      "source": [
        "# given an image and a heatmap showing the importance of each pixels, this function destructs the image such that only a specified percentage of\n",
        "# the most important pixels remains, and the other ones get replaced by salt and pepper noise\n",
        "def destruct_image(img, heatmap, percentage=0.2):\n",
        "    result = img.copy()\n",
        "    idx_list = top_n_indexes(heatmap, int(percentage*(28*28)))\n",
        "    idx_list.sort(key = lambda tup: tup[0])\n",
        "    other_pixels = get_other_pixels(idx_list)\n",
        "    for idx in other_pixels:\n",
        "        #random_val = random.sample([0,1], 1)[0]\n",
        "        random_val = 0\n",
        "        result[idx[0],idx[1],0] = random_val\n",
        "        result[idx[0],idx[1],1] = random_val\n",
        "        result[idx[0],idx[1],2] = random_val\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__m5Wsn3-eMp"
      },
      "source": [
        "new_x_test = []\n",
        "for idx, curr_img in enumerate(data[2][indices]):\n",
        "    curr_heatmap = analysis[idx, 0]\n",
        "    new_img = destruct_image(curr_img, curr_heatmap, 0.2)\n",
        "    new_x_test.append(new_img)\n",
        "new_x_test = np.array(new_x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk5Ev2hC-3Uc"
      },
      "source": [
        "new_predictions = model.predict(new_x_test)\n",
        "new_predictions = list(np.argmax(new_predictions, axis=1))\n",
        "true_labels = list(data[3][:num_samples])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BB6PQUn-4n2",
        "outputId": "6212fcad-1145-40a9-e085-0b8074222d51"
      },
      "source": [
        "accuracy_score(true_labels, new_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.145"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}